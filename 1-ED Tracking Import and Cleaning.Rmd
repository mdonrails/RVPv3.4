---
title: "ED Tracking Import and Cleaning"
output: html_notebook
---


```{r include=FALSE}
#Import data, save original df, load packages
library(readxl)
track <- read_excel("~/Library/Mobile Documents/com~apple~CloudDocs/Documents/EM Research/Data Analysis/RVPv3.4/RawData/all.ED.visits.Oct17toSept19.xlsx", 
    col_types = c("text", "text", "date", 
        "text", "text", "date", "date", "date", 
        "date", "date", "date", "date", "date", 
        "date", "date", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric"))
View(track)

saveRDS(track, file = "track.raw.Rda")

library(tidyverse)
library(lubridate)
```


##Disposition Distribution
Sorted from highest to lowest
```{r}
track$Dispo_ED <- factor(track$Dispo_ED) %>% fct_explicit_na() #convert to factors

track %>% count(Dispo_ED, sort = T)
```

##Sex Distribution
```{r}
track$Sex <- factor(track$Sex) #convert to factors
track %>% count(Sex, sort = T)
```

##Descriptive Stats

###LOS by Dispo
```{r}
library(psych)
do.call("rbind", describeBy(track$CheckinToCheckout.mins, track$Dispo_ED)) %>% 
  rownames_to_column(var="Dispo") %>% 
  arrange(desc(n))
```

###Time Durations
```{r}
track %>% 
  select(ends_with(".mins")) %>%
  summarise_all(funs(
                min = min,
                mean = mean,
                median = median,
                max = max,
                sd = sd),
                na.rm = T) %>% #has too wide of df
  gather(stat, val) %>% #gather all columns as "stat", values as "val"
  separate(stat, into = c("var", "stat"), sep = "_") %>% #parse out stat as sep column, using "_" to separate
  spread(stat, val) %>%  #convert stat into multiple columns
  select(var, min, mean, median, max, sd) #change order
```


```{r include=FALSE}
#Export Cleaned Tracking Data
saveRDS(track, file = "track.clean.Rda")
```


```{r}
# Analyse duplicates
track.dup <- track[duplicated(track$FIN)|duplicated(track$FIN, fromLast = TRUE),] %>% arrange(FIN) #show encounters with duplicate FIN

track.dup$NaCount <- track.dup %>% is.na() %>% rowSums() #create NA count to choose b/w duplicates

track.dup$lowLOS <- ifelse(track.dup$CheckinToCheckout.mins < 20, 1, 0) # create low LOS binary (of <20 mins) to choose b/w duplicates

track.dup$highLOS <- ifelse(track.dup$CheckinToCheckout.mins > (24*60), 1, 0) # create high LOS binary (of >1 day) to choose b/w duplicates

track.dup <- track.dup %>% arrange(FIN, lowLOS, highLOS, NaCount)
```

```{r}
#remove duplicates
track$NaCount <- track %>% is.na() %>% rowSums() #create NA count to choose b/w duplicates

track$lowLOS <- ifelse(track$CheckinToCheckout.mins < 20, 1, 0) # create low LOS binary (of <20 mins) to choose b/w duplicates

track$highLOS <- ifelse(track$CheckinToCheckout.mins > (24*60), 1, 0) # create high LOS binary (of >1 day) to choose b/w duplicates

track <- track %>% arrange(FIN, lowLOS, highLOS, NaCount) #sort 

track.dedup <- track %>% distinct(FIN, .keep_all = TRUE) #remove duplicates (removes 2nd row)

#Export Cleaned Tracking Data
saveRDS(track.dedup, file = "dfs/track.dedup.Rda")
```

